{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3a8372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xxhash\n",
    "import json\n",
    "import numpy as np\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "from transformers import AutoTokenizer\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79a6695",
   "metadata": {},
   "outputs": [],
   "source": [
    "fonts = [f.name for f in fm.fontManager.ttflist]\n",
    "print(fonts)\n",
    "print(\"可用字体数量:\", len(fonts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6945d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "@lru_cache(maxsize=10)\n",
    "def _cache_json(path: Path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def _compute_hash(token_ids: list[int]):\n",
    "    \"\"\"\n",
    "    计算完整 prompt token 序列的哈希值\n",
    "    \"\"\"\n",
    "    h = xxhash.xxh64()\n",
    "    h.update(np.array(token_ids).tobytes())\n",
    "    return h.intdigest()\n",
    "\n",
    "def read_jsonl(path: str | Path) -> list:\n",
    "    \"\"\"读取 JSONL 文件并返回 list\"\"\"\n",
    "    file_path = Path(path)\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        print(f\"文件不存在: {file_path}\")\n",
    "        raise FileNotFoundError(f\"文件不存在: {file_path}\")\n",
    "\n",
    "    data_list = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():  # 跳过空行\n",
    "                data_list.append(json.loads(line))\n",
    "    return data_list\n",
    "\n",
    "def serialize_token_ids(token_ids: list[int] | np.ndarray) -> str:\n",
    "    \"\"\"将 token_ids 序列化为 [数值,数值,数值] 格式\"\"\"\n",
    "    if isinstance(token_ids, np.ndarray):\n",
    "        return str(token_ids.tolist())\n",
    "    else:\n",
    "        return str(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b03648",
   "metadata": {},
   "outputs": [],
   "source": [
    "drafter_path = Path(\"/root/.cache/modelscope/hub/models/Qwen/Qwen3-0___6B\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(drafter_path)\n",
    "prompt_list = read_jsonl(\"/root/nano-vllm/select_question.jsonl\")\n",
    "verify_logits_path = Path(\"/root/nano-vllm/tmp/Qwen3-0___6B\")\n",
    "target_logits_path = Path(\"/root/nano-vllm/tmp/Qwen3-4B\")\n",
    "\n",
    "current_path = Path.cwd()\n",
    "output_dir = current_path / \"plt\"\n",
    "if not output_dir.exists():\n",
    "    output_dir.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1456b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1\n",
    "topk = 5\n",
    "for i in tqdm.tqdm(range(0,len(prompt_list),step)):\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": prompt_list[i][\"turns\"][0]}],\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=True\n",
    "        )\n",
    "    hash_id = _compute_hash(prompt)\n",
    "    \n",
    "    draft_logits =  read_jsonl(verify_logits_path / f\"seq_{hash_id}_draft.jsonl\")\n",
    "    target_data = json.load(open(target_logits_path / f\"seq_{hash_id}.json\"))\n",
    "\n",
    "    target_logits = target_data[\"logits\"]\n",
    "    misalign_logits = len(draft_logits)/len(target_logits)\n",
    "    print(f\"misalign_logits: {misalign_logits} for seq {hash_id}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb107d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1\n",
    "topk = 5\n",
    "for i in tqdm.tqdm(range(0,len(prompt_list),step)):\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": prompt_list[i][\"turns\"][0]}],\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=True\n",
    "        )\n",
    "    hash_id = _compute_hash(prompt)\n",
    "    \n",
    "    draft_logits =  read_jsonl(logits_path / f\"seq_{hash_id}_draft.jsonl\")\n",
    "    target_logits = read_jsonl(logits_path / f\"seq_{hash_id}_target.jsonl\")\n",
    "\n",
    "    assert len(draft_logits) == len(target_logits)\n",
    "    for j in range(len(draft_logits)):\n",
    "        draft_token_id = np.argmax(draft_logits[j])\n",
    "        draft_topk_token_ids = np.argpartition(draft_logits[j], -topk)[-topk:]\n",
    "        target_token_id = np.argmax(target_logits[j])\n",
    "        target_topk_token_ids = np.argpartition(target_logits[j], -topk)[-topk:]\n",
    "        \n",
    "        output_path = output_dir / f\"logits_{hash_id}_target_{target_topk_token_ids}_draft_{draft_topk_token_ids}.png\"\n",
    "        if output_path.exists():\n",
    "            print(f\"{output_path} exists\")\n",
    "            continue\n",
    "        \n",
    "        # print(f\"draft_token_id: {draft_token_id}\")\n",
    "        # print(f\"draft_topk_token_ids: {draft_topk_token_ids}\")\n",
    "        # test = [draft_logits[j][p] for p in draft_topk_token_ids]\n",
    "        # print(f\"{test}\")\n",
    "        # assert draft_token_id == draft_topk_token_ids[-1]\n",
    "        # assert target_token_id == target_topk_token_ids[-1]\n",
    "\n",
    "        draft_token = tokenizer.decode(draft_topk_token_ids[-1])\n",
    "        target_token = tokenizer.decode(target_topk_token_ids[-1])\n",
    "    \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        plt.style.use('default')\n",
    "        # https://matplotlib.org/stable/gallery/style_sheets/style_sheets_reference.html\n",
    "        plt.vlines(range(len(draft_logits[0])), ymin=0, ymax=draft_logits[0],\n",
    "                    color=\"C0\",lw=0.5,alpha=0.3,\n",
    "                    label=f\"draft\"\n",
    "                    )\n",
    "        plt.vlines(range(len(target_logits[0])), ymin=0, ymax=target_logits[0],\n",
    "                    color=\"C1\",lw=0.5,alpha=0.3,\n",
    "                    label=f\"target\"\n",
    "                    )\n",
    "        plt.xlabel('token id')\n",
    "        plt.ylabel('logits')\n",
    "        plt.legend(fontsize=10)\n",
    "        plt.savefig(output_dir / f\"logits_{hash_id}_target_{target_topk_token_ids}_draft_{draft_topk_token_ids}.png\")\n",
    "        # plt.show() 会显示图形并清空当前的图形对象\n",
    "        # plt.show()\n",
    "        print(f\"{output_path} dumped\")\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
